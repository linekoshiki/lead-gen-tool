#!/usr/bin/env python3
"""
lead_collector.py
Google Mapsã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Webã‚µã‚¤ãƒˆã‚’è§£æã—ã¦è©³ç´°æƒ…å ±ã‚’åé›†ã™ã‚‹ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ã€‚
"""
import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import datetime
import re
from urllib.parse import urlparse

async def analyze_website(page, url):
    """
    Webã‚µã‚¤ãƒˆã‚’è¨ªå•ã—ã¦SNSãƒªãƒ³ã‚¯ã‚„å„ç¨®ãƒ•ã‚©ãƒ¼ãƒ ã€Webã‚«ã‚¿ãƒ­ã‚°ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
    """
    info = {
        "sns": [],
        "has_form": False,
        "catalog_types": set(),
        "remarks": []
    }
    
    if not url or url == "ãªã—":
        return info

    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚¢ã‚¯ã‚»ã‚¹
        await page.goto(url, timeout=15000, wait_until="domcontentloaded")
        
        # ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
        links = await page.evaluate('''() => {
            return Array.from(document.querySelectorAll('a')).map(a => {
                return { href: a.href, text: a.innerText };
            });
        }''')
        
        # SNSæ¤œçŸ¥ (ç•¥)
        sns_domains = {
            "twitter.com": "Twitter",
            "x.com": "X",
            "facebook.com": "Facebook",
            "instagram.com": "Instagram",
            "youtube.com": "YouTube",
            "linkedin.com": "LinkedIn",
            "line.me": "LINE"
        }
        
        found_sns = set()
        for link_obj in links:
            link = link_obj["href"]
            for domain, name in sns_domains.items():
                if domain in link and name not in found_sns:
                    found_sns.add(name)
        info["sns"] = list(found_sns)
        
        # ãƒ•ã‚©ãƒ¼ãƒ æ¤œçŸ¥ (ç•¥)
        content = await page.content()
        form_keywords = ['contact', 'inquiry', 'form', 'ãŠå•ã„åˆã‚ã›', 'ãŠå•åˆã›', 'ç›¸è«‡', 'ç”³è¾¼']
        
        if any(k in url.lower() for k in form_keywords):
            info["has_form"] = True
        elif "<form" in content.lower():
             if "submit" in content.lower() or "é€ä¿¡" in content:
                 info["has_form"] = True
        else:
            for link_obj in links:
                if any(k in link_obj["href"].lower() or k in link_obj["text"].lower() for k in form_keywords):
                    info["has_form"] = True
                    break

        # Webã‚«ã‚¿ãƒ­ã‚°æ¤œçŸ¥ (è©³ç´°åŒ–)
        catalog_keywords = ['catalog', 'catalogue', 'ã‚«ã‚¿ãƒ­ã‚°', 'é›»å­ã‚«ã‚¿ãƒ­ã‚°', 'ãƒ‡ã‚¸ã‚¿ãƒ«ã‚«ã‚¿ãƒ­ã‚°', 'å†Šå­']
        for link_obj in links:
            l_href = link_obj["href"].lower()
            l_text = link_obj["text"].lower()
            
            if any(k in l_href or k in l_text for k in catalog_keywords):
                # PDFåˆ¤å®š
                if ".pdf" in l_href:
                    info["catalog_types"].add("PDF")
                # é›»å­bookåˆ¤å®š (book, viewer, ebook, digital-bookãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰)
                elif any(k in l_href for k in ["book", "viewer", "ebook", "digital"]) or any(k in l_text for k in ["é›»å­", "ãƒ‡ã‚¸ã‚¿ãƒ«"]):
                    info["catalog_types"].add("book")
                # ãã®ä»–ã‚«ã‚¿ãƒ­ã‚°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚ã‚‹ãŒå½¢å¼ä¸æ˜ãªå ´åˆã‚‚ä¸€å¿œãƒã‚§ãƒƒã‚¯
                elif any(k in l_text for k in catalog_keywords):
                    # ãƒ†ã‚­ã‚¹ãƒˆã«ã€Œé›»å­ã€ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°é›»å­bookã€ãã†ã§ãªã‘ã‚Œã°ä¸€æ—¦ã‚«ã‚¿ãƒ­ã‚°ã¨ã—ã¦æ‰±ã†ãŒ
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ã€ŒPDFã€ã‹ã€Œbookã€ã«å¯„ã›ã‚‹
                    if "é›»å­" in l_text or "ãƒ‡ã‚¸ã‚¿ãƒ«" in l_text:
                        info["catalog_types"].add("book")
                    else:
                        # åˆ¤å®šãŒã¤ã‹ãªã„å ´åˆã¯PDFã‹é›»å­bookã®ã©ã¡ã‚‰ã‹ã§ã‚ã‚Œã°è‰¯ã„ãŒ
                        # å¤šãã®å ´åˆã¯é›»å­bookãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã¸ã®èª˜å°ãªã®ã§ã€Œbookã€å¯„ã‚Š
                        info["catalog_types"].add("book")
                    
    except Exception as e:
        info["remarks"].append(f"Webã‚µã‚¤ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
    
    return info

async def collect_leads(keyword, max_results=20, progress_callback=None):
    """
    Google Mapsã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’åé›† + Webã‚µã‚¤ãƒˆè§£æ
    """
    leads = []
    
    def report_progress(current, total, status):
        if progress_callback:
            progress_callback(current, total, status)
    
    async with async_playwright() as p:
        report_progress(0, max_results, "ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­... (Webè§£æãƒ¢ãƒ¼ãƒ‰)")
        if max_results > 50:
            # å¤§é‡å–å¾—æ™‚ã¯ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹æ¨å¥¨
            browser = await p.chromium.launch(headless=True)
        else:
             # å°‘é‡ãªã‚‰ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„ã‚ˆã†ã«è¦‹ãˆã‚‹å ´åˆã‚‚ã‚ã‚‹ãŒã€å®‰å®šæ€§ã®ãŸã‚True
            browser = await p.chromium.launch(headless=True)
            
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        # Google Mapsã‚’é–‹ã„ã¦æ¤œç´¢
        report_progress(0, max_results, f"ğŸ” Google Mapsã§ã€Œ{keyword}ã€ã‚’æ¤œç´¢ä¸­...")
        await page.goto(f"https://www.google.com/maps/search/{keyword}")
        
        try:
            await page.wait_for_selector('a.hfpxzc', timeout=10000)
        except:
            report_progress(0, max_results, "âŒ çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†
        report_progress(0, max_results, "ğŸ“œ ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ä¸­...")
        scroll_attempts = 0
        while len(await page.query_selector_all('a.hfpxzc')) < max_results and scroll_attempts < 15:
            feed = await page.query_selector('div[role="feed"]')
            if feed:
                await page.evaluate('(el) => el.scrollBy(0, 5000)', feed)
            else:
                await page.mouse.wheel(0, 5000)
            await asyncio.sleep(2)
            scroll_attempts += 1
            
        articles = await page.query_selector_all('a.hfpxzc')
        total_to_process = min(len(articles), max_results)
        report_progress(0, total_to_process, f"âœ… {total_to_process}ä»¶ã®å€™è£œã‚’å–å¾—ã€‚è©³ç´°è§£æã‚’é–‹å§‹ã—ã¾ã™...")

        # è§£æç”¨ãƒšãƒ¼ã‚¸ï¼ˆã‚¿ãƒ–ï¼‰ã‚’ä½œæˆã—ã¦ä¸¦è¡Œå‡¦ç†ã£ã½ãã™ã‚‹æ‰‹ã‚‚ã‚ã‚‹ãŒã€ä»Šå›ã¯ç›´åˆ—ã§ç¢ºå®Ÿã«
        analysis_page = await context.new_page()

        for i, article in enumerate(articles[:total_to_process]):
            try:
                name = await article.get_attribute("aria-label")
                report_progress(i + 1, total_to_process, f"ğŸ¢ [{i+1}/{total_to_process}] {name} ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—ä¸­...")
                
                # è©³ç´°å–å¾—ã‚¯ãƒªãƒƒã‚¯
                await article.click()
                await asyncio.sleep(1.5)
                
                # --- åŸºæœ¬æƒ…å ±å–å¾— ---
                # æ¥­ç¨® (ã‚«ãƒ†ã‚´ãƒª)
                industry_elem = await page.query_selector('button.DkEaL')
                industry = await industry_elem.inner_text() if industry_elem else "ä¸æ˜"
                
                # ä½æ‰€
                address_elem = await page.query_selector('button[data-item-id="address"]')
                address = await address_elem.get_attribute("aria-label") if address_elem else "ä¸æ˜"
                address = address.replace("ä½æ‰€: ", "").strip()

                # é›»è©±
                phone_elem = await page.query_selector('button[data-item-id^="phone:tel:"]')
                phone = await phone_elem.get_attribute("aria-label") if phone_elem else "ä¸æ˜"
                phone = phone.replace("é›»è©±ç•ªå·: ", "").strip()
                
                # Webã‚µã‚¤ãƒˆ
                website_elem = await page.query_selector('a[data-item-id="authority"]')
                website = await website_elem.get_attribute("href") if website_elem else "ãªã—"
                
                # --- Webã‚µã‚¤ãƒˆè©³ç´°è§£æ ---
                web_info = {"sns": [], "has_form": False, "catalog_types": set(), "remarks": []}
                if website != "ãªã—":
                    report_progress(i + 1, total_to_process, f"ğŸŒ {name} ã®Webã‚µã‚¤ãƒˆã‚’è§£æä¸­...")
                    web_info = await analyze_website(analysis_page, website)
                
                # Webã‚«ã‚¿ãƒ­ã‚°ã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                catalog_text = "ãªã—/ä¸æ˜"
                if web_info["catalog_types"]:
                    catalog_text = ", ".join(sorted(list(web_info["catalog_types"])))

                leads.append({
                    "ä¼æ¥­å": name,
                    "æ¥­ç¨®": industry,
                    "ä½æ‰€": address,
                    "é›»è©±ç•ªå·": phone,
                    "Webã‚µã‚¤ãƒˆ": website,
                    "å•åˆã›ãƒ•ã‚©ãƒ¼ãƒ ": "ã‚ã‚Š" if web_info["has_form"] else "ãªã—/ä¸æ˜",
                    "SNS": ", ".join(web_info["sns"]) if web_info["sns"] else "ãªã—",
                    "Webã‚«ã‚¿ãƒ­ã‚°": catalog_text,
                    "å‚™è€ƒ": " ".join(web_info["remarks"]),
                    "åé›†æ—¥": datetime.datetime.now().strftime("%Y-%m-%d")
                })
                
            except Exception as e:
                report_progress(i + 1, total_to_process, f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)[:20]}...ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                continue

        await analysis_page.close()
        await browser.close()
        
        report_progress(len(leads), len(leads), "ğŸ‰ å…¨ä»¶ã®åé›†ã¨è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    return leads

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    res = asyncio.run(collect_leads("äº¬éƒ½å¸‚ å¸æ³•æ›¸å£«", max_results=3))
    print(res)
